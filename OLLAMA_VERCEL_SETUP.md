# ğŸŒ Usar Ollama en Vercel - GuÃ­a Completa

## ğŸ¯ Objetivo

Hacer que Vercel use tu Ollama local (RTX 3080) para **$0.00 de costo API**, incluso en producciÃ³n.

## âš™ï¸ CÃ³mo Funciona

```
Vercel (ProducciÃ³n)
       â†“
   Internet
       â†“
  TÃºnel HTTPS â† Cloudflare Tunnel / Ngrok
       â†“
Tu PC (localhost:11434)
       â†“
  Ollama + RTX 3080
```

## ğŸš€ OpciÃ³n 1: Cloudflare Tunnel (RECOMENDADO)

### âœ… Ventajas
- âœ… **100% GRATIS** (sin lÃ­mites)
- âœ… No requiere cuenta
- âœ… URLs persistentes opcionales
- âœ… MÃ¡s rÃ¡pido que Ngrok
- âœ… No expira

### ğŸ“¦ InstalaciÃ³n

```bash
# Windows
winget install Cloudflare.cloudflared

# Verificar instalaciÃ³n
cloudflared --version
```

### ğŸ”§ Uso RÃ¡pido (TÃºnel Temporal)

```bash
# 1. AsegÃºrate que Ollama estÃ© corriendo
ollama serve

# 2. En otra terminal, inicia el tÃºnel
cloudflared tunnel --url http://localhost:11434
```

**VerÃ¡s algo como:**
```
2025-11-11T06:00:00Z INF +--------------------------------------------------------------------------------------------+
2025-11-11T06:00:00Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |
2025-11-11T06:00:00Z INF |  https://abc-def-ghi-jkl.trycloudflare.com                                                |
2025-11-11T06:00:00Z INF +--------------------------------------------------------------------------------------------+
```

### ğŸŒ Configurar en Vercel

1. Copia la URL: `https://abc-def-ghi-jkl.trycloudflare.com`

2. Ve a Vercel â†’ Tu Proyecto â†’ Settings â†’ Environment Variables

3. Agrega:
   ```
   OLLAMA_BASE_URL=https://abc-def-ghi-jkl.trycloudflare.com/v1
   ```

4. Redeploy tu proyecto

### ğŸ”’ Uso Permanente (TÃºnel con Nombre)

```bash
# 1. Login a Cloudflare (gratis)
cloudflared tunnel login

# 2. Crear tÃºnel con nombre
cloudflared tunnel create ollama

# 3. Configurar ruta
cloudflared tunnel route dns ollama ollama.yourdomain.com

# 4. Correr tÃºnel permanente
cloudflared tunnel run ollama
```

Ahora tendrÃ¡s: `https://ollama.yourdomain.com` (permanente)

---

## ğŸš€ OpciÃ³n 2: Ngrok

### âœ… Ventajas
- âœ… FÃ¡cil de usar
- âœ… UI web para ver trÃ¡fico
- âœ… URLs personalizadas con cuenta gratuita

### âš ï¸ Desventajas
- âš ï¸ Free tier: 1 agent, 1 endpoint, 40 conexiones/min
- âš ï¸ URL cambia cada vez (a menos que pagues)

### ğŸ“¦ InstalaciÃ³n

```bash
# Windows
winget install ngrok.ngrok

# Crear cuenta gratis: https://ngrok.com/
# Copiar authtoken de tu dashboard

# Configurar authtoken
ngrok config add-authtoken YOUR_AUTHTOKEN
```

### ğŸ”§ Uso

```bash
# Iniciar tÃºnel
ngrok http 11434
```

**VerÃ¡s:**
```
Forwarding  https://abcd-1234-efgh-5678.ngrok-free.app -> http://localhost:11434
```

### ğŸŒ Configurar en Vercel

```
OLLAMA_BASE_URL=https://abcd-1234-efgh-5678.ngrok-free.app/v1
```

---

## ğŸš€ OpciÃ³n 3: VPS con Ollama (PRODUCCIÃ“N)

### âœ… Ventajas
- âœ… Siempre disponible (no depende de tu PC)
- âœ… MÃ¡s rÃ¡pido (servidor dedicado)
- âœ… Escalable (puedes usar GPU cloud)

### ğŸ’° Costo
- Hetzner GPU: ~â‚¬50/mes (RTX 4000)
- DigitalOcean GPU: ~$150/mes
- RunPod: ~$0.30/hora (solo cuando se usa)

### ğŸ”§ Setup RÃ¡pido

```bash
# En tu VPS (Ubuntu)
curl https://ollama.ai/install.sh | sh

# Exponer Ollama a internet
sudo nano /etc/systemd/system/ollama.service

# Cambiar:
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Reiniciar
sudo systemctl restart ollama

# Configurar Nginx con SSL (Let's Encrypt)
sudo apt install nginx certbot python3-certbot-nginx
sudo certbot --nginx -d ollama.yourdomain.com
```

### ğŸŒ Configurar en Vercel

```
OLLAMA_BASE_URL=https://ollama.yourdomain.com/v1
```

---

## ğŸ› ï¸ Script AutomÃ¡tico

Usa nuestro script helper:

```bash
npm run ollama:setup-tunnel
```

Este script:
1. âœ… Verifica que Ollama estÃ© corriendo
2. âœ… Te muestra las 3 opciones
3. âœ… Puede iniciar Cloudflare Tunnel automÃ¡ticamente
4. âœ… Te da la URL para copiar a Vercel

---

## ğŸ§ª Verificar que Funciona

### 1. Test Local del TÃºnel

```bash
# Si tu tÃºnel es: https://abc.trycloudflare.com
curl https://abc.trycloudflare.com/api/tags
```

DeberÃ­as ver tus modelos Ollama.

### 2. Test desde Vercel

DespuÃ©s de deployar con `OLLAMA_BASE_URL` configurado:

```bash
# En tu app, ve a /api/courses/generate y genera un curso
# Verifica los logs en Vercel Dashboard
```

DeberÃ­as ver:
```
[LLM Fallback] ğŸ¯ Ollama added as PRIMARY provider (remote (https://...), zero cost)
[LLM Fallback] âœ… Ollama is running and ready (REMOTE TUNNEL - ZERO COST)
[LLM] ğŸ  Using remote Ollama model: llama3.2:3b (ZERO API COST)
```

---

## ğŸ”’ Seguridad (IMPORTANTE)

### âš ï¸ TÃºneles PÃºblicos son INSEGUROS por defecto

Cualquiera con la URL puede usar tu Ollama. **Opciones:**

### OpciÃ³n A: API Key en Headers

1. Configura Ollama con autenticaciÃ³n (requiere proxy inverso)

```nginx
# nginx.conf
location /v1/ {
    if ($http_authorization != "Bearer YOUR_SECRET_KEY") {
        return 401;
    }
    proxy_pass http://localhost:11434/v1/;
}
```

2. En Vercel:
```
OLLAMA_BASE_URL=https://your-tunnel.com/v1
OLLAMA_API_KEY=YOUR_SECRET_KEY
```

### OpciÃ³n B: IP Whitelist

Solo permite IPs de Vercel:

```nginx
# nginx.conf
location /v1/ {
    # Vercel IPs: https://vercel.com/docs/edge-network/regions#ip-addresses
    allow 76.76.21.0/24;
    allow 76.76.21.98;
    deny all;
    
    proxy_pass http://localhost:11434/v1/;
}
```

### OpciÃ³n C: Cloudflare Access (GRATIS)

AÃ±ade autenticaciÃ³n con email a tu tÃºnel:

```bash
cloudflared tunnel --url http://localhost:11434 --name ollama --access-policy email:tu@email.com
```

---

## ğŸ“Š ComparaciÃ³n de Opciones

| MÃ©todo | Costo | Setup | Permanente | Seguridad | Velocidad |
|--------|-------|-------|------------|-----------|-----------|
| **Cloudflare (temp)** | ğŸ†“ Gratis | âš¡ 1 min | âŒ No | âš ï¸ PÃºblica | ğŸš€ RÃ¡pida |
| **Cloudflare (named)** | ğŸ†“ Gratis | ğŸ”§ 10 min | âœ… SÃ­ | ğŸ”’ Con Access | ğŸš€ RÃ¡pida |
| **Ngrok (free)** | ğŸ†“ Gratis | âš¡ 2 min | âŒ No | âš ï¸ PÃºblica | ğŸ¢ Media |
| **Ngrok (paid)** | ğŸ’° $8/mes | âš¡ 2 min | âœ… SÃ­ | ğŸ”’ Con auth | ğŸš€ RÃ¡pida |
| **VPS** | ğŸ’° â‚¬50/mes | ğŸ”§ 30 min | âœ… SÃ­ | ğŸ”’ Total | ğŸš€ğŸš€ Muy rÃ¡pida |

---

## ğŸ¯ RecomendaciÃ³n

### Para Testing/Desarrollo:
**â†’ Cloudflare Tunnel temporal** (gratis, 1 minuto setup)

### Para ProducciÃ³n:
**â†’ Cloudflare Tunnel con nombre + Cloudflare Access** (gratis, seguro)

### Para Alta Disponibilidad:
**â†’ VPS con Ollama** (costo mensual, siempre disponible)

---

## ğŸ’¡ Pro Tips

### 1. Keep Alive

Usa `pm2` o `systemd` para que el tÃºnel siempre estÃ© activo:

```bash
# Instalar pm2
npm install -g pm2

# Iniciar tÃºnel con pm2
pm2 start cloudflared -- tunnel --url http://localhost:11434

# Guardar para auto-start
pm2 save
pm2 startup
```

### 2. Monitoreo

Agrega logging para ver uso:

```bash
# En tu .env.local
LOG_OLLAMA_REQUESTS=true
```

### 3. Fallback Cloud

Si tu PC estÃ¡ apagado, el sistema automÃ¡ticamente usarÃ¡ Anthropic/Groq:

```
[LLM Fallback] âš ï¸ Ollama not responding, skipping to cloud providers
[LLM Fallback] ğŸ¤– Trying provider: ANTHROPIC
```

---

## ğŸ†˜ Troubleshooting

### TÃºnel conectado pero Vercel no lo alcanza

**Causa:** Firewall bloqueando

**SoluciÃ³n:**
```bash
# Windows: Permitir trÃ¡fico entrante en puerto 11434
netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=TCP localport=11434
```

### "Connection refused" desde Vercel

**Causa:** Ollama solo escucha en localhost

**SoluciÃ³n:**
```bash
# Configurar Ollama para escuchar en 0.0.0.0
$env:OLLAMA_HOST="0.0.0.0:11434"
ollama serve
```

### TÃºnel muy lento

**Causa:** Cloudflare free tier puede tener latencia

**Soluciones:**
1. Usa Ngrok con cuenta paga (mÃ¡s rÃ¡pido)
2. Cambia a VPS en regiÃ³n cercana a Vercel (us-east-1)

---

## ğŸ“ Variables de Entorno en Vercel

```bash
# Obligatorio
OLLAMA_BASE_URL=https://your-tunnel-url.com/v1

# Opcional (si configuraste autenticaciÃ³n)
OLLAMA_API_KEY=your_secret_key

# Opcional (para debugging)
LOG_OLLAMA_REQUESTS=true
```

**Recuerda:** DespuÃ©s de agregar variables, debes hacer **Redeploy** del proyecto.

---

## âœ… Checklist Final

- [ ] Ollama corriendo en localhost:11434
- [ ] TÃºnel iniciado (Cloudflare/Ngrok)
- [ ] URL HTTPS copiada
- [ ] `OLLAMA_BASE_URL` configurado en Vercel
- [ ] Proyecto redeployado
- [ ] Test de generaciÃ³n de curso exitoso
- [ ] Logs muestran "Using remote Ollama model"
- [ ] Costo API = $0.00 âœ¨

---

## ğŸ‰ Resultado

**Vercel ahora usa tu RTX 3080 local = $0.00 API costs en producciÃ³n** ğŸš€
