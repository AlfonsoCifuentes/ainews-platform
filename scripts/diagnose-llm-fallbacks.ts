/**
 * Script de diagnÃ³stico del sistema de fallbacks LLM
 * Verifica quÃ© providers estÃ¡n disponibles y en quÃ© orden se intentarÃ¡n
 */

import { config } from 'dotenv';
import { resolve } from 'path';
import { getAvailableProviders } from '../lib/ai/llm-client';

// Load environment variables from .env.local
config({ path: resolve(process.cwd(), '.env.local') });

async function diagnose() {
  console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
  console.log('â•‘       ğŸ” DIAGNÃ“STICO DEL SISTEMA DE FALLBACKS LLM             â•‘');
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

  // Verificar Ollama
  console.log('1ï¸âƒ£  OLLAMA (Local Model - ZERO COST)');
  console.log('â”€'.repeat(60));

  const isVercel = process.env.VERCEL === '1';
  if (isVercel) {
    console.log('   âš ï¸  Running on Vercel - Ollama not available');
  } else {
    const ollamaUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    try {
      const response = await fetch(`${ollamaUrl}/api/tags`, {
        signal: AbortSignal.timeout(3000)
      });
      
      if (response.ok) {
        const data = await response.json();
        console.log(`   âœ… Ollama is RUNNING`);
        console.log(`   ğŸ“ URL: ${ollamaUrl}`);
        console.log(`   ğŸ¤– Available models:`);
        data.models?.forEach((model: { name: string; size: number }) => {
          const sizeMB = (model.size / 1024 / 1024).toFixed(0);
          console.log(`      â€¢ ${model.name} (${sizeMB} MB)`);
        });
      } else {
        console.log(`   âŒ Ollama responded but returned status: ${response.status}`);
      }
    } catch {
      console.log(`   âŒ Ollama is NOT running`);
      console.log(`   ğŸ’¡ Start it with: ollama serve`);
      console.log(`   ğŸ’¡ Or install: winget install Ollama.Ollama`);
    }
  }

console.log('\n2ï¸âƒ£  CLOUD PROVIDERS (API Keys)');
console.log('â”€'.repeat(60));

const providers = [
  { name: 'Anthropic Claude', env: 'ANTHROPIC_API_KEY', prefix: 'sk-ant-' },
  { name: 'Groq', env: 'GROQ_API_KEY', prefix: 'gsk_' },
  { name: 'Google Gemini', env: 'GEMINI_API_KEY', prefix: 'AIza' },
  { name: 'DeepSeek', env: 'DEEPSEEK_API_KEY', prefix: 'sk-' },
  { name: 'Mistral AI', env: 'MISTRAL_API_KEY', prefix: '' },
  { name: 'OpenRouter', env: 'OPENROUTER_API_KEY', prefix: 'sk-or-' },
  { name: 'Together AI', env: 'TOGETHER_API_KEY', prefix: '' },
];

let _configuredCount = 0;

providers.forEach(({ name, env, prefix }) => {
  const apiKey = process.env[env];
  if (apiKey) {
    const preview = prefix 
      ? `${prefix}...${apiKey.slice(-4)}`
      : `${apiKey.substring(0, 10)}...${apiKey.slice(-4)}`;
    console.log(`   âœ… ${name.padEnd(20)} ${preview}`);
    _configuredCount++;
  } else {
    console.log(`   âŒ ${name.padEnd(20)} Not configured`);
  }
});

console.log('\n3ï¸âƒ£  FALLBACK ORDER (Priority)');
console.log('â”€'.repeat(60));

const availableProviders = getAvailableProviders();

if (availableProviders.length === 0) {
  console.log('   âŒ NO PROVIDERS AVAILABLE!');
  console.log('   âš ï¸  Course generation will FAIL');
} else {
  console.log(`   âœ… ${availableProviders.length} provider(s) available:\n`);
  availableProviders.forEach((provider, index) => {
    const isFree = provider === 'ollama';
    const emoji = isFree ? 'ğŸ ' : 'â˜ï¸';
    const cost = isFree ? 'FREE (Local)' : 'API Cost';
    const priority = index === 0 ? 'ğŸ¥‡ PRIMARY' : index === 1 ? 'ğŸ¥ˆ SECONDARY' : 'ğŸ¥‰ TERTIARY';
    console.log(`   ${priority} ${emoji} ${provider.toUpperCase().padEnd(12)} - ${cost}`);
  });
}

console.log('\n4ï¸âƒ£  RECOMMENDATIONS');
console.log('â”€'.repeat(60));

if (availableProviders.length === 0) {
  console.log('   â›” CRITICAL: No LLM providers available!');
  console.log('   ğŸ“ Actions required:');
  console.log('      1. Install Ollama: winget install Ollama.Ollama');
  console.log('      2. Or add API keys to .env.local');
} else if (availableProviders.length === 1) {
  console.log('   âš ï¸  Only 1 provider available - no fallback redundancy');
  console.log('   ğŸ’¡ Recommendation: Add at least 2 more API keys for reliability');
} else if (availableProviders.length === 2) {
  console.log('   âœ… 2 providers available - basic redundancy');
  console.log('   ğŸ’¡ Recommendation: Add 1 more for better reliability');
} else {
  console.log('   âœ… Excellent! Multiple providers configured');
  console.log('   ğŸ¯ System will try providers in order until one succeeds');
}

if (availableProviders[0] === 'ollama') {
  console.log('   ğŸ† OPTIMAL: Using Ollama as primary = ZERO API costs!');
} else if (availableProviders.includes('ollama')) {
  console.log('   âš ï¸  Ollama is available but not primary (check if it\'s running)');
} else {
  console.log('   ğŸ’¡ TIP: Install Ollama for free local LLM (no API costs)');
}

console.log('\n5ï¸âƒ£  QUICK TEST');
console.log('â”€'.repeat(60));

if (availableProviders.length > 0) {
  console.log('   ğŸ“ To test course generation:');
  console.log('      1. Go to: http://localhost:3000/en/courses');
  console.log('      2. Click "Generate Course"');
  console.log('      3. Topic: "Introduction to Neural Networks"');
  console.log('      4. Check server logs for fallback sequence');
  console.log('\n   ğŸ“Š Expected log output:');
  console.log('      [LLM Fallback] ğŸ”„ Starting multi-provider fallback...');
  availableProviders.slice(0, 3).forEach((provider) => {
    console.log(`      [LLM Fallback] ğŸ¤– Trying provider: ${provider.toUpperCase()}`);
  });
} else {
  console.log('   âŒ Cannot test - no providers available');
}

console.log('\n' + 'â•'.repeat(64));
console.log('   ğŸ’¡ For more info, see: COURSE_GENERATION_FIX.md');
console.log('â•'.repeat(64) + '\n');
}

// Run the diagnostic
diagnose().catch(console.error);
