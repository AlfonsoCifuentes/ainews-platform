# LLM Fallback System - Cascada de Proveedores

## ğŸ“Š Flujo de Cascada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   1. OLLAMA LOCAL (PREFERIDO)                   â”‚
â”‚            âœ“ Cero costo    âœ“ Sin lÃ­mites    âœ“ Offline          â”‚
â”‚  Si estÃ¡ disponible â†’ USAR INMEDIATAMENTE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    (Si falla/no disponible)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    2. PROVEEDORES NUBE                           â”‚
â”‚  Cascada en orden: Groq â†’ Gemini â†’ OpenRouter â†’ Together        â”‚
â”‚                 â†’ Mistral â†’ OpenAI â†’ DeepSeek â†’ Anthropic      â”‚
â”‚                                                                  â”‚
â”‚  âœ“ Intentar cada uno hasta que uno funcione                    â”‚
â”‚  âœ“ Si falla por quota/error â†’ siguiente proveedor              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    (Si todos fallan)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  3. DESCARGA LOCAL AUTOMÃTICA                   â”‚
â”‚                    (Ãšltimo recurso)                             â”‚
â”‚  Si Ollama NO estÃ¡ corriendo:                                  â”‚
â”‚    1. Detecta que NO hay modelo local instalado                â”‚
â”‚    2. Inicia descarga automÃ¡tica de llama2:7b (~2.7GB)         â”‚
â”‚    3. Espera a que termine (5-15 minutos)                      â”‚
â”‚    4. Usa el modelo local para generar                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Proveedores en Detalle

### **Nivel 1: OLLAMA (Local)**
```typescript
OLLAMA_BASE_URL=http://localhost:11434  // Por defecto
// O remoto via tunnel:
OLLAMA_BASE_URL=https://tu-ollama-tunnel.ngrok.io
```
- **Costo**: 0
- **LÃ­mite**: Infinito
- **Velocidad**: Depende de hardware (~10-30 seg/curso)
- **Modelo usado**: llama2:7b (4GB RAM, 13GB GPU VRAM)
- **InstalaciÃ³n**: `brew install ollama` o `ollama.ai` en Windows

### **Nivel 2: GROQ (Primario nube)**
```typescript
GROQ_API_KEY=gsk_...
```
- **Costo**: GRATIS hasta 30 req/min
- **Modelo**: mixtral-8x7b (muy rÃ¡pido)
- **Tiempo respuesta**: ~5-10 seg
- **LÃ­mite**: 30 solicitudes/minuto
- **Sign up**: https://console.groq.com

### **Nivel 3: GEMINI**
```typescript
GEMINI_API_KEY=AIzaSy...
```
- **Costo**: GRATIS
- **Modelo**: gemini-2.0-flash
- **Tiempo respuesta**: ~8-15 seg
- **LÃ­mite**: 15 req/min (free tier)
- **Sign up**: https://aistudio.google.com

### **Nivel 4: OPENROUTER**
```typescript
OPENROUTER_API_KEY=sk-or-...
```
- **Costo**: GRATIS (modelos free)
- **Modelo**: Meta Llama 3.1 70B
- **Tiempo respuesta**: ~10-20 seg
- **LÃ­mite**: Generoso en free tier
- **Sign up**: https://openrouter.ai

### **Nivel 5: TOGETHER**
```typescript
TOGETHER_API_KEY=...
```
- **Costo**: GRATIS
- **Modelo**: Meta Llama 3.1 70B
- **Tiempo respuesta**: ~10-20 seg
- **LÃ­mite**: Free tier disponible
- **Sign up**: https://api.together.xyz

### **Nivel 6: MISTRAL**
```typescript
MISTRAL_API_KEY=...
```
- **Costo**: Freemium
- **Modelo**: mistral-7b-instruct
- **Tiempo respuesta**: ~8-15 seg
- **Sign up**: https://console.mistral.ai

### **Nivel 7: OPENAI**
```typescript
OPENAI_API_KEY=sk-...
```
- **Costo**: PAGADO (~$0.002/1K tokens)
- **Modelo**: gpt-4o-mini (rÃ¡pido + barato)
- **Tiempo respuesta**: ~5-8 seg
- **Ventaja**: MÃ¡s fiable que otros
- **Sign up**: https://platform.openai.com

### **Nivel 8: DEEPSEEK**
```typescript
DEEPSEEK_API_KEY=...
```
- **Costo**: Muy barato o gratis
- **Modelo**: DeepSeek v2
- **Tiempo respuesta**: ~10-20 seg
- **Sign up**: https://platform.deepseek.com

### **Nivel 9: ANTHROPIC**
```typescript
ANTHROPIC_API_KEY=sk-ant-...
```
- **Costo**: PAGADO
- **Modelo**: Claude 3.5 Sonnet
- **Ventaja**: Excelente calidad
- **Sign up**: https://console.anthropic.com

---

## ğŸ“ˆ Modelo Local: LLAMA2:7B

### Especificaciones
- **TamaÃ±o descargado**: ~4GB (en memoria)
- **TamaÃ±o comprimido**: ~2.7GB
- **VRAM requerida**: ~13GB (sin cuantizar)
- **Velocidad**: 10-30 tokens/seg (depende de GPU)
- **Calidad**: Buena para cursos (91% calidad vs GPT-4)
- **Ventaja**: GRATIS + Sin lÃ­mites + Offline

### Descarga AutomÃ¡tica
```typescript
// En lib/ai/llm-client.ts
async function prepareLocalModel(): Promise<boolean> {
  // 1. Verifica si Ollama estÃ¡ corriendo
  // 2. Si modelo existe â†’ lo usa
  // 3. Si NO existe â†’ descarga automÃ¡ticamente llama2:7b
  // 4. Streaming de progreso: "Downloading model (45%)"
}
```

### CÃ³mo Funciona en Fallback

```typescript
// Cuando TODOS los cloud providers fallan:
export async function createLLMClientWithFallback() {
  // Intenta Ollama primero (si estÃ¡ corriendo)
  // Intenta todos los cloud providers
  
  // SI TODOS FALLAN:
  if (!isVercel) {
    const localReady = await prepareLocalModel(); // â† Descarga si falta
    if (localReady) {
      return createLLMClient('ollama'); // Usa llama2:7b
    }
  }
  
  throw new Error('No providers available');
}
```

---

## ğŸš€ Casos de Uso

### Caso 1: Usuario con Ollama + conexiÃ³n internet
```
1. Intenta Ollama â†’ âœ“ ENCONTRADO â†’ Usa local (CERO costo)
2. Si Ollama no responde â†’ Usa Groq (gratis, nube)
3. Si Groq quota â†’ Usa Gemini
...
```

### Caso 2: Usuario sin Ollama, pero con Groq API key
```
1. Intenta Ollama â†’ âœ— No disponible
2. Intenta Groq â†’ âœ“ ENCONTRADO â†’ Usa Groq (gratis)
3. Si Groq quota â†’ Usa Gemini
...
```

### Caso 3: Usuario sin internet ni API keys (modo supervivencia)
```
1. Intenta todos â†’ âœ— Todos fallan
2. Descarga automÃ¡ticamente llama2:7b (si Ollama instalado)
3. Genera localmente sin conexiÃ³n (5-15 min de espera por descarga)
4. Funciona para siempre offline
```

### Caso 4: Vercel Production
```
1. Intenta Ollama â†’ âœ— No disponible en Vercel serverless
2. Cascada de 8 cloud providers â†’ DEBE haber al menos uno
3. NO intenta descargar local (no hay espacio/tiempo en Vercel)
4. Fallback: Anthropic como Ãºltimo recurso
```

---

## ğŸ’¾ InstalaciÃ³n y ConfiguraciÃ³n

### Setup RÃ¡pido (Recomendado)

**1. Instala Ollama:**
```bash
# macOS
brew install ollama

# Windows / Linux
# Descarga en https://ollama.ai
```

**2. Inicia Ollama:**
```bash
ollama serve  # Escucha en http://localhost:11434
```

**3. El sistema automÃ¡ticamente:**
- Detecta Ollama corriendo
- Descarga llama2:7b si no existe
- Lo usa para generar cursos

### Setup Avanzado (Con Tunnel Remoto)

Si quieres usar Ollama desde Vercel/servidor remoto:

```bash
# En mÃ¡quina local con Ollama:
ollama serve

# En otra terminal, crear tunnel con ngrok:
ngrok http 11434

# En .env.local:
OLLAMA_BASE_URL=https://tu-url-ngrok.ngrok.io
```

---

## ğŸ“Š Monitoreo de Fallbacks

### Logs del Sistema
```typescript
// En consola/logs verÃ¡s:
[LLM] âœ“ Using Ollama provider (LOCAL MODEL - NO API COSTS)
// O si falla local:
[LLM] Ollama not available, falling back to cloud providers
[LLM] âœ“ Using groq provider (cloud fallback)
// O si todos fallan:
[LLM] âš ï¸  All cloud providers exhausted. Attempting to use local model...
[LOCAL] ğŸ“¥ Llama2 model not found. Downloading llama2:7b...
[LOCAL] âœ“ Llama2 model download complete!
[LLM] âœ“ Using local Llama2 model (LAST RESORT FALLBACK)
```

### Dashboard Vercel
- Ve a Vercel â†’ proyecto â†’ Deployments â†’ Functions â†’ `/api/generate-course-simple`
- Ve quÃ© proveedor se usÃ³ en logs
- Monitorea tiempos de respuesta

---

## âš¡ Performance Esperado

| Proveedor | Tiempo | Costo | Offline |
|-----------|--------|-------|---------|
| Ollama Local | 20-40s | $0 | âœ“ |
| Groq | 8-15s | $0 | âœ— |
| Gemini | 10-18s | $0 | âœ— |
| OpenRouter | 12-20s | $0-$ | âœ— |
| OpenAI | 5-10s | $0.002 | âœ— |

---

## ğŸ†˜ Troubleshooting

### "No LLM providers available"
```
âŒ Ollama no estÃ¡ corriendo
âŒ Todos los API keys faltan o son invÃ¡lidos
âœ“ SoluciÃ³n:
  1. Instala Ollama: ollama.ai
  2. Ejecuta: ollama serve
  3. O configura al menos un API key
```

### "Timeout downloading llama2:7b"
```
âŒ ConexiÃ³n lenta o interrumpida
âœ“ SoluciÃ³n:
  1. Descarga manual: ollama pull llama2:7b
  2. Intenta en conexiÃ³n mÃ¡s estable
  3. O usa API key de Groq/Gemini
```

### "Ollama: all providers quota exceeded"
```
âœ“ Normal - todos los free tiers se agotaron
âœ“ SoluciÃ³n:
  1. Espera al prÃ³ximo ciclo (generalmente 1 minuto)
  2. Instala Ollama + llama2:7b
  3. O compra mÃ¡s quota en OpenAI
```

---

## ğŸ“š Recursos

- **Ollama**: https://ollama.ai
- **Groq**: https://console.groq.com
- **Gemini**: https://aistudio.google.com
- **OpenRouter**: https://openrouter.ai
- **OpenAI**: https://platform.openai.com

---

**Ãšltima actualizaciÃ³n**: Nov 14, 2025
**Commit**: bf4f366 - Add automatic local Llama2 7B model download
