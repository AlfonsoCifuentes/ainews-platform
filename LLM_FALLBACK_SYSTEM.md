# üîÑ Sistema de Fallback Multi-Proveedor LLM

## üìã Resumen

Se ha implementado un **sistema robusto de fallback** que intenta **TODOS los proveedores LLM disponibles** antes de fallar, con **mensajes de error detallados y accionables**.

## üéØ Problema Resuelto

**ANTES:**
- ‚ùå Solo usaba un proveedor LLM a la vez
- ‚ùå Si ese proveedor ten√≠a rate limit ‚Üí error 500 gen√©rico
- ‚ùå Mensaje de error poco √∫til: "Failed to generate course"
- ‚ùå No intentaba otros proveedores disponibles

**AHORA:**
- ‚úÖ Intenta **TODOS** los proveedores configurados secuencialmente
- ‚úÖ Cada proveedor tiene 2 intentos con backoff exponencial
- ‚úÖ Errores **categorizados y detallados** (rate_limit, auth, timeout, network, etc.)
- ‚úÖ Mensajes **accionables** que explican qu√© hacer
- ‚úÖ Solo falla cuando **TODOS** los proveedores han sido agotados

---

## ü§ñ Proveedores Disponibles (En Orden de Prioridad)

La plataforma ahora intenta estos proveedores en este orden:

1. **Anthropic Claude** (`ANTHROPIC_API_KEY`) - Mejor para respuestas JSON estructuradas
2. **DeepSeek** (`DEEPSEEK_API_KEY`) - Alta calidad, proveedor chino, econ√≥mico
3. **Mistral AI** (`MISTRAL_API_KEY`) - Alta calidad, proveedor europeo
4. **Google Gemini** (`GEMINI_API_KEY`) - Gemini 2.0 Flash
5. **OpenRouter** (`OPENROUTER_API_KEY`) - Gateway multi-proveedor
6. **Groq** (`GROQ_API_KEY`) - Inferencia r√°pida
7. **Together AI** (`TOGETHER_API_KEY`) - Modelos Meta Llama

**üí° Recomendaci√≥n:** Configura **al menos 2-3 proveedores** para m√°xima redundancia.

---

## üîß Nueva Arquitectura

### 1. Clasificaci√≥n de Errores (`classifyLLMError`)

Cada error se clasifica en:

| Tipo | Descripci√≥n | Retryable | HTTP Status |
|------|-------------|-----------|-------------|
| `rate_limit` | L√≠mite de uso excedido | ‚úÖ S√≠ | 429 |
| `auth` | Error de autenticaci√≥n/API key | ‚ùå No | 503 |
| `timeout` | Request tard√≥ demasiado | ‚úÖ S√≠ | 504 |
| `network` | No se puede alcanzar el proveedor | ‚úÖ S√≠ | 503 |
| `validation` | Respuesta malformada | ‚úÖ S√≠ | 500 |
| `config` | Proveedor no configurado | ‚ùå No | 503 |
| `unknown` | Error desconocido | ‚úÖ S√≠ | 500 |

### 2. Sistema de Fallback (`classifyWithAllProviders`)

```typescript
// Intenta TODOS los proveedores disponibles
for (const provider of ['anthropic', 'deepseek', 'mistral', ...]) {
  try {
    // Crea cliente para este proveedor
    const client = createLLMClient(provider);
    
    for (let attempt = 1; attempt <= 2; attempt++) {
      try {
        // Intenta la clasificaci√≥n/generaci√≥n
        const result = await client.classify(prompt, schema, systemPrompt);
        return { result, provider, attempts };
      } catch (error) {
        const errorInfo = classifyLLMError(error);
        
        // Si no es retryable (auth error), salta al siguiente proveedor
        if (!errorInfo.retryable) break;
        
        // Espera antes del retry (500ms * attempt)
        await sleep(500 * attempt);
      }
    }
  } catch (clientError) {
    // Proveedor no disponible, contin√∫a con el siguiente
    continue;
  }
}

// Si todos fallan, genera reporte detallado
throw new DetailedError(...);
```

### 3. Mensajes de Error Mejorados

#### Ejemplo de Error de Rate Limit (Todos los Proveedores Agotados):

```
‚ùå Course generation failed - all 3 AI providers exhausted after 6 attempts.

üìã FAILURE DETAILS:
  ‚Ä¢ anthropic (attempt 1): RATE_LIMIT - Rate limit exceeded. 429 Too Many Requests
  ‚Ä¢ anthropic (attempt 2): RATE_LIMIT - Rate limit exceeded. 429 Too Many Requests
  ‚Ä¢ deepseek (attempt 1): RATE_LIMIT - Rate limit exceeded. quota exceeded
  ‚Ä¢ deepseek (attempt 2): RATE_LIMIT - Rate limit exceeded. quota exceeded
  ‚Ä¢ gemini (attempt 1): RATE_LIMIT - Rate limit exceeded. 429 Resource exhausted
  ‚Ä¢ gemini (attempt 2): RATE_LIMIT - Rate limit exceeded. 429 Resource exhausted

üí° RECOMMENDED ACTIONS:
  ‚è∞ RATE LIMIT: You've hit usage limits. Wait 5-10 minutes or add more API keys for redundancy.

üîß TROUBLESHOOTING:
  1. Check your API keys are correctly configured in .env.local
  2. Verify you haven't exceeded free tier limits
  3. Try again in a few minutes (rate limits reset)
  4. Add more API keys for better redundancy

Available providers: anthropic, deepseek, gemini
```

#### Ejemplo de Error de Configuraci√≥n:

```
‚ùå CRITICAL: No LLM providers configured!

Please add at least one API key to your .env.local file:
  ‚Ä¢ ANTHROPIC_API_KEY=sk-ant-... (Recommended - best for JSON)
  ‚Ä¢ DEEPSEEK_API_KEY=sk-... (High quality, affordable)
  ‚Ä¢ MISTRAL_API_KEY=... (European provider, high quality)
  ‚Ä¢ GEMINI_API_KEY=... (Google Gemini)
  ‚Ä¢ OPENROUTER_API_KEY=sk-or-... (Multi-provider gateway)
  ‚Ä¢ GROQ_API_KEY=... (Fast inference)
  ‚Ä¢ TOGETHER_API_KEY=... (Meta models)

Get free API keys from:
  ‚Ä¢ Anthropic: https://console.anthropic.com/
  ‚Ä¢ DeepSeek: https://platform.deepseek.com/
  ‚Ä¢ Mistral: https://console.mistral.ai/
  ‚Ä¢ Google AI Studio: https://aistudio.google.com/
  ‚Ä¢ OpenRouter: https://openrouter.ai/
  ‚Ä¢ Groq: https://console.groq.com/
  ‚Ä¢ Together AI: https://api.together.xyz/
```

---

## üìä Respuestas HTTP Espec√≠ficas

El endpoint `/api/courses/generate` ahora devuelve c√≥digos HTTP apropiados:

| Error Type | HTTP Status | User Message |
|------------|-------------|--------------|
| Rate Limit | 429 | "‚è∞ Rate limit exceeded. All AI providers are currently at capacity. Please try again in 5-10 minutes." |
| Auth Error | 503 | "üîë Authentication error. The AI service configuration is invalid. Please contact the administrator." |
| Timeout | 504 | "‚è±Ô∏è Request timeout. The AI service took too long to respond. Try a simpler course topic or try again later." |
| Network Error | 503 | "üåê Network error. Cannot reach AI services. Please check your connection and try again." |
| Config Error | 503 | "‚öôÔ∏è Configuration error. AI services are not properly configured. Please contact the administrator." |
| Validation Error | 500 | "‚ö†Ô∏è AI response validation failed. The AI returned malformed data. This is usually temporary - please try again." |

---

## üî¨ Ejemplo de Logs en Consola

```
[Course Generator 2025-01-09T10:30:00.000Z] üöÄ NEW COURSE GENERATION REQUEST STARTED
[Course Generator 2025-01-09T10:30:00.000Z] ‚úÖ Parameters validated
[Course Generator 2025-01-09T10:30:00.000Z] üìä Available providers: ['anthropic', 'deepseek', 'mistral', 'gemini']
[Course Generator 2025-01-09T10:30:00.000Z] ‚úÖ Step 5/8: Found 4 available providers: anthropic, deepseek, mistral, gemini
[Course Generator 2025-01-09T10:30:00.000Z] ü§ñ Provider fallback order: anthropic ‚Üí deepseek ‚Üí mistral ‚Üí gemini

[LLM Fallback] üîÑ Starting multi-provider fallback with 4 providers available
[LLM Fallback] üìã Provider order: anthropic ‚Üí deepseek ‚Üí mistral ‚Üí gemini

[LLM Fallback] ü§ñ Trying provider: ANTHROPIC
[LLM Fallback] ‚úÖ anthropic client initialized
[LLM Fallback] üîÑ anthropic attempt 1/2...
[LLM Fallback] ‚ùå anthropic attempt 1/2 failed:
[LLM Fallback]    Type: rate_limit
[LLM Fallback]    Message: Rate limit exceeded. 429 Too Many Requests
[LLM Fallback]    Retryable: true
[LLM Fallback] ‚è≥ Waiting 500ms before retry...
[LLM Fallback] üîÑ anthropic attempt 2/2...
[LLM Fallback] ‚ùå anthropic attempt 2/2 failed:
[LLM Fallback]    Type: rate_limit
[LLM Fallback]    Message: Rate limit exceeded. 429 Too Many Requests
[LLM Fallback]    Retryable: true
[LLM Fallback] ‚ö†Ô∏è  anthropic exhausted all 2 attempts, trying next provider...

[LLM Fallback] ü§ñ Trying provider: DEEPSEEK
[LLM Fallback] ‚úÖ deepseek client initialized
[LLM Fallback] üîÑ deepseek attempt 1/2...
[LLM Fallback] ‚úÖ SUCCESS with deepseek on attempt 1!
[LLM Fallback] üìä Total attempts across all providers: 3

[Course Generator 2025-01-09T10:30:05.000Z] ‚úÖ Course outline created successfully with deepseek!
```

---

## üéì Uso en el C√≥digo

### Generaci√≥n de Outline:

```typescript
const { result: outline, provider: outlineProvider } = await classifyWithProviderFallback(
  outlinePrompt,
  CourseOutlineSchema,
  JSON_SYSTEM_PROMPT,
);

console.log(`‚úÖ Course outline created successfully with ${outlineProvider}!`);
```

### Generaci√≥n de M√≥dulos:

```typescript
for (const moduleOutline of outline.modules) {
  const { result: moduleContent } = await classifyWithProviderFallback(
    modulePrompt,
    ModuleContentSchema,
    JSON_SYSTEM_PROMPT,
  );
  // moduleContent ahora tiene el contenido generado
}
```

### Traducci√≥n:

```typescript
async function translateCourse(
  sourceLocale: 'en' | 'es',
  targetLocale: 'en' | 'es',
  course: CourseContentBundle
): Promise<CourseTranslation> {
  const { result } = await classifyWithProviderFallback(
    prompt, 
    CourseTranslationSchema, 
    JSON_SYSTEM_PROMPT
  );
  return result;
}
```

---

## üìà Ventajas del Nuevo Sistema

### 1. **M√°xima Disponibilidad**
- Si un proveedor falla, autom√°ticamente intenta el siguiente
- M√∫ltiples proveedores = m√∫ltiples oportunidades de √©xito
- Reduce dr√°sticamente la tasa de error

### 2. **Errores Accionables**
- El usuario sabe **exactamente** qu√© pas√≥
- Sabe **cu√°ndo** reintentar (rate limit) vs **cu√°ndo** no tiene sentido (config error)
- Los administradores ven logs detallados con clasificaci√≥n de errores

### 3. **Resiliencia Autom√°tica**
- No se necesita intervenci√≥n manual
- El sistema se auto-recupera si un proveedor vuelve a estar disponible
- Backoff exponencial evita sobrecargar proveedores

### 4. **Observabilidad**
- Todos los intentos se loggean
- Se guarda en `ai_system_logs` con el proveedor usado
- M√©tricas: intentos totales, proveedor exitoso, tiempo de ejecuci√≥n

### 5. **Escalabilidad**
- F√°cil a√±adir nuevos proveedores
- Solo requiere agregar API key al `.env.local`
- El sistema los detecta y usa autom√°ticamente

---

## üîë Configuraci√≥n de API Keys

### Archivo `.env.local`:

```bash
# === LLM Providers (Configure at least 2-3 for redundancy) ===

# Anthropic Claude (Recommended - Best for JSON)
ANTHROPIC_API_KEY=sk-ant-api03-...

# DeepSeek (High quality, affordable)
DEEPSEEK_API_KEY=sk-...

# Mistral AI (European provider, high quality)
MISTRAL_API_KEY=...

# Google Gemini
GEMINI_API_KEY=...

# OpenRouter (Multi-provider gateway)
OPENROUTER_API_KEY=sk-or-v1-...

# Groq (Fast inference)
GROQ_API_KEY=gsk_...

# Together AI (Meta models)
TOGETHER_API_KEY=...
```

### Obtener API Keys Gratis:

| Proveedor | URL | Tier Gratuito |
|-----------|-----|---------------|
| Anthropic | https://console.anthropic.com/ | Cr√©ditos iniciales |
| DeepSeek | https://platform.deepseek.com/ | Rate limit generoso |
| Mistral | https://console.mistral.ai/ | Cr√©ditos de prueba |
| Google Gemini | https://aistudio.google.com/ | Tier gratuito permanente |
| OpenRouter | https://openrouter.ai/ | $1 de cr√©dito gratis |
| Groq | https://console.groq.com/ | Rate limit gratuito |
| Together AI | https://api.together.xyz/ | Cr√©ditos de prueba |

---

## üöÄ Testing

### Probar Rate Limit Handling:

1. Configura solo 1 proveedor
2. Genera m√∫ltiples cursos seguidos hasta agotar el rate limit
3. Verifica que el error sea claro y accionable

### Probar Multi-Provider Fallback:

1. Configura 3+ proveedores
2. Genera un curso
3. Verifica en los logs qu√© proveedor fue usado
4. Si el primero falla, verifica que intente el siguiente

### Probar Error Messages:

1. Elimina todos los API keys ‚Üí Error de configuraci√≥n
2. Usa API key inv√°lido ‚Üí Error de autenticaci√≥n
3. Desconecta internet ‚Üí Error de red

---

## üìù Archivo Modificados

### 1. `lib/ai/llm-client.ts`
- ‚úÖ A√±adido `classifyLLMError()` - Clasificaci√≥n de errores
- ‚úÖ A√±adido `classifyWithAllProviders()` - Sistema de fallback
- ‚úÖ A√±adido `generateActionableAdvice()` - Consejos espec√≠ficos por error

### 2. `app/api/courses/generate/route.ts`
- ‚úÖ Eliminado uso de `createLLMClientWithFallback()` (cliente √∫nico)
- ‚úÖ Reemplazado `classifyWithRetry()` con `classifyWithProviderFallback()`
- ‚úÖ A√±adida clasificaci√≥n de errores en catch principal
- ‚úÖ HTTP status codes espec√≠ficos por tipo de error
- ‚úÖ Mensajes de error user-friendly basados en error type
- ‚úÖ Logging de proveedor usado en `ai_system_logs`

---

## üéØ Pr√≥ximos Pasos Sugeridos

1. **Monitoreo de Proveedores:**
   - Dashboard en `/admin` mostrando qu√© proveedores est√°n activos
   - Tasa de √©xito por proveedor
   - Tiempo promedio de respuesta

2. **Rate Limit Tracking:**
   - Guardar en DB cu√°ndo se alcanz√≥ rate limit
   - Estimar cu√°ndo se resetea
   - Sugerir al usuario cu√°ndo reintentar

3. **Provider Priority Tuning:**
   - Ordenar proveedores por tasa de √©xito
   - Preferir proveedores m√°s r√°pidos/baratos primero
   - Permitir configuraci√≥n manual de prioridad

4. **Retry con Jitter:**
   - A√±adir randomizaci√≥n al backoff para evitar thundering herd
   - Implementar circuit breaker por proveedor

---

## ‚úÖ Checklist de Implementaci√≥n

- [x] Clasificaci√≥n de errores (`classifyLLMError`)
- [x] Sistema de fallback multi-proveedor (`classifyWithAllProviders`)
- [x] Mensajes de error detallados y accionables
- [x] HTTP status codes espec√≠ficos
- [x] Logging del proveedor usado
- [x] Logging de errores en `ai_system_logs`
- [x] Reemplazo de `classifyWithRetry` en outline generation
- [x] Reemplazo de `classifyWithRetry` en module generation
- [x] Reemplazo de `classifyWithRetry` en translation
- [x] Testing de compilaci√≥n
- [ ] Testing end-to-end en producci√≥n
- [ ] Documentaci√≥n para usuarios finales

---

**¬°El sistema ahora es MUCHO m√°s robusto contra rate limits y fallos de proveedores!** üéâ
