# ğŸ§  LLM en el Navegador del Usuario - $0 API Costs

## ğŸ¯ FilosofÃ­a

**NO tunneling, NO servidor propio** â†’ El modelo corre **EN EL NAVEGADOR DEL USUARIO**

## ğŸš€ Opciones Disponibles

### â­ OpciÃ³n 1: Transformers.js (RECOMENDADO)

**QuÃ© es**: Ejecuta modelos Hugging Face directamente en el navegador con WebGPU/WASM

**Ventajas**:
- âœ… **100% gratis** (modelo descarga 1 vez, cache permanente)
- âœ… **CERO latencia** (todo local)
- âœ… **Privacidad total** (nada sale del navegador)
- âœ… **Funciona offline** despuÃ©s de primera descarga
- âœ… **WebGPU = ultra rÃ¡pido** en RTX 3080, M1/M2, etc.

**Desventajas**:
- âš ï¸ Primera carga: ~200MB-2GB (depende del modelo)
- âš ï¸ Requiere navegador moderno (Chrome 113+, Edge 113+)

#### ğŸ“¦ ImplementaciÃ³n

```bash
npm install @xenova/transformers
```

```typescript
// lib/ai/browser-llm.ts
import { pipeline, env } from '@xenova/transformers';

// Configure cache
env.allowLocalModels = false;
env.useBrowserCache = true;

export class BrowserLLM {
  private generator: any = null;
  
  async initialize(
    modelId: string = 'Xenova/Phi-3.5-mini-instruct', // 3.8GB
    onProgress?: (progress: number) => void
  ) {
    this.generator = await pipeline(
      'text-generation',
      modelId,
      {
        progress_callback: (data: any) => {
          if (data.status === 'progress') {
            onProgress?.(data.progress);
          }
        }
      }
    );
  }
  
  async generate(prompt: string): Promise<string> {
    if (!this.generator) {
      throw new Error('Model not initialized. Call initialize() first.');
    }
    
    const result = await this.generator(prompt, {
      max_new_tokens: 2000,
      temperature: 0.7,
      do_sample: true,
    });
    
    return result[0].generated_text;
  }
}
```

**Uso en componente**:

```tsx
// app/[locale]/courses/generate/page.tsx
'use client';

import { useState } from 'react';
import { BrowserLLM } from '@/lib/ai/browser-llm';

export default function CourseGeneratePage() {
  const [llm] = useState(() => new BrowserLLM());
  const [downloadProgress, setDownloadProgress] = useState(0);
  const [isReady, setIsReady] = useState(false);
  
  const handleInitialize = async () => {
    await llm.initialize(
      'Xenova/Phi-3.5-mini-instruct',
      (progress) => setDownloadProgress(progress)
    );
    setIsReady(true);
  };
  
  const handleGenerate = async () => {
    const course = await llm.generate(`
      Generate a course about Machine Learning.
      Output JSON with: { title, modules: [...] }
    `);
    console.log(JSON.parse(course));
  };
  
  return (
    <div>
      {!isReady ? (
        <button onClick={handleInitialize}>
          Download AI Model ({downloadProgress}%)
        </button>
      ) : (
        <button onClick={handleGenerate}>
          Generate Course (100% Free, Runs in Your Browser)
        </button>
      )}
    </div>
  );
}
```

#### ğŸ¯ Modelos Recomendados

| Modelo | TamaÃ±o | Calidad | Velocidad | Uso |
|--------|--------|---------|-----------|-----|
| **Phi-3.5-mini** | 3.8GB | â­â­â­â­â­ | ğŸš€ğŸš€ğŸš€ | GeneraciÃ³n cursos |
| **TinyLlama** | 637MB | â­â­â­ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ | ResÃºmenes, clasificaciÃ³n |
| **Qwen2-1.5B** | 1.5GB | â­â­â­â­ | ğŸš€ğŸš€ğŸš€ğŸš€ | Balance perfecto |
| **SmolLM-360M** | 360MB | â­â­ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ | Ultra ligero |

**ConversiÃ³n de modelos**:
```bash
# Convertir cualquier modelo de Hugging Face a ONNX (Transformers.js compatible)
npx transformers-cli convert --model mistralai/Mistral-7B-v0.1
```

---

### â­ OpciÃ³n 2: WebLLM (Chrome Built-in AI)

**QuÃ© es**: Usa el modelo **Gemini Nano** que viene **preinstalado en Chrome**

**Ventajas**:
- âœ… **CERO descarga** (viene en Chrome 127+)
- âœ… **Ultra rÃ¡pido** (optimizado por Google)
- âœ… **100% gratis**
- âœ… **Privacidad total**

**Desventajas**:
- âš ï¸ Solo Chrome 127+ con flag activado
- âš ï¸ Modelo pequeÃ±o (~1B parÃ¡metros)
- âš ï¸ API experimental (puede cambiar)

#### ğŸ“¦ ImplementaciÃ³n

```typescript
// lib/ai/chrome-ai.ts
export class ChromeAI {
  private session: any = null;
  
  async initialize() {
    // @ts-expect-error - Experimental API
    if (!window.ai || !window.ai.languageModel) {
      throw new Error('Chrome AI not available. Enable chrome://flags/#optimization-guide-on-device-model');
    }
    
    // @ts-expect-error
    this.session = await window.ai.languageModel.create({
      temperature: 0.7,
      topK: 3,
    });
  }
  
  async generate(prompt: string): Promise<string> {
    if (!this.session) {
      throw new Error('Session not initialized');
    }
    
    return await this.session.prompt(prompt);
  }
  
  async generateStream(
    prompt: string,
    onChunk: (text: string) => void
  ): Promise<void> {
    const stream = this.session.promptStreaming(prompt);
    
    for await (const chunk of stream) {
      onChunk(chunk);
    }
  }
}
```

**Activar en Chrome**:
1. Ir a `chrome://flags/#optimization-guide-on-device-model`
2. Activar "Optimization Guide On Device Model"
3. Reiniciar Chrome

---

### â­ OpciÃ³n 3: WebGPU + GGUF (MÃ¡ximo Control)

**QuÃ© es**: Ejecuta modelos GGUF (Llama.cpp format) directamente en WebGPU

**Ventajas**:
- âœ… **Modelos ultra comprimidos** (Q4_K_M = 4 bits)
- âœ… **WebGPU = velocidad nativa** en GPUs modernas
- âœ… **Compatible con ANY modelo** (Llama, Mistral, Gemma, etc.)

**Desventajas**:
- âš ï¸ MÃ¡s complejo de implementar
- âš ï¸ Requiere WebGPU (Chrome 113+)

#### ğŸ“¦ ImplementaciÃ³n

```bash
npm install @mlc-ai/web-llm
```

```typescript
// lib/ai/webgpu-llm.ts
import * as webllm from '@mlc-ai/web-llm';

export class WebGPU_LLM {
  private engine: webllm.MLCEngine | null = null;
  
  async initialize(
    modelId: string = 'Llama-3.2-3B-Instruct-q4f32_1-MLC',
    onProgress?: (report: any) => void
  ) {
    this.engine = await webllm.CreateMLCEngine(
      modelId,
      {
        initProgressCallback: (report) => {
          onProgress?.(report);
          console.log(report.text);
        },
      }
    );
  }
  
  async generate(prompt: string): Promise<string> {
    if (!this.engine) throw new Error('Not initialized');
    
    const reply = await this.engine.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7,
      max_tokens: 2000,
    });
    
    return reply.choices[0].message.content || '';
  }
  
  async *generateStream(prompt: string) {
    if (!this.engine) throw new Error('Not initialized');
    
    const stream = await this.engine.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      stream: true,
      temperature: 0.7,
    });
    
    for await (const chunk of stream) {
      const delta = chunk.choices[0]?.delta?.content;
      if (delta) yield delta;
    }
  }
}
```

**Modelos disponibles**: https://mlc.ai/web-llm/#chat-demo

---

### â­ OpciÃ³n 4: APIs Gratuitas (Sin Descarga)

**Si el usuario NO quiere descargar nada**, usar APIs gratuitas:

#### A. Hugging Face Inference API (GRATIS)

```typescript
// lib/ai/hf-api.ts
export async function generateWithHF(prompt: string): Promise<string> {
  const response = await fetch(
    'https://api-inference.huggingface.co/models/microsoft/Phi-3.5-mini-instruct',
    {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.NEXT_PUBLIC_HF_TOKEN}`, // Token gratis
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        inputs: prompt,
        parameters: {
          max_new_tokens: 2000,
          temperature: 0.7,
        },
      }),
    }
  );
  
  const data = await response.json();
  return data[0].generated_text;
}
```

**Free tier**: 30,000 requests/month

#### B. Together.ai (GRATIS)

```typescript
// Ya lo tienes configurado!
// Free tier: $25 credits = ~500,000 tokens
```

#### C. Groq (ULTRA RÃPIDO + GRATIS)

```typescript
// Ya lo tienes configurado!
// Free tier: 14,400 requests/day (Llama 3.1 70B)
```

---

## ğŸ¯ Arquitectura HÃ­brida Recomendada

**Combina lo mejor de ambos mundos**:

```typescript
// lib/ai/hybrid-llm.ts
export class HybridLLM {
  private browserLLM: BrowserLLM | null = null;
  private cloudAvailable = true;
  
  async generateCourse(topic: string): Promise<Course> {
    // 1. Intenta con modelo del navegador (si estÃ¡ descargado)
    if (this.browserLLM) {
      try {
        return await this.browserLLM.generate(topic);
      } catch (err) {
        console.log('Browser LLM failed, falling back to cloud');
      }
    }
    
    // 2. Fallback a cloud (Groq â†’ Together â†’ HF)
    return await this.generateWithCloud(topic);
  }
  
  async offerBrowserDownload(): Promise<boolean> {
    const userWants = await confirm(
      'Â¿Descargar modelo AI en tu navegador? (3.8GB, 100% privado, funciona offline)'
    );
    
    if (userWants) {
      this.browserLLM = new BrowserLLM();
      await this.browserLLM.initialize('Xenova/Phi-3.5-mini-instruct');
      return true;
    }
    
    return false;
  }
}
```

**UX Flow**:

```tsx
// Primera visita
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¯ Generar Curso de Machine Learningâ”‚
â”‚                                     â”‚
â”‚ âš¡ OpciÃ³n 1: Usar API Cloud         â”‚
â”‚    â†’ InstantÃ¡neo, requiere internet â”‚
â”‚                                     â”‚
â”‚ ğŸ§  OpciÃ³n 2: Descargar Modelo (3.8GB)â”‚
â”‚    â†’ Primera vez tarda, luego GRATISâ”‚
â”‚    â†’ Funciona offline despuÃ©s       â”‚
â”‚    â†’ 100% privado                   â”‚
â”‚                                     â”‚
â”‚ [Usar Cloud] [Descargar Modelo]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

// Si descarga modelo
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“¥ Descargando Phi-3.5-mini...      â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 67% (2.5GB / 3.8GB) â”‚
â”‚                                     â”‚
â”‚ Esto solo pasa 1 vez. DespuÃ©s es    â”‚
â”‚ instantÃ¡neo y 100% gratis.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

// Visitas posteriores
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Modelo descargado                â”‚
â”‚ ğŸ¯ Generar curso (100% gratis)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š ComparaciÃ³n de Opciones

| MÃ©todo | Costo | Setup Usuario | Offline | Privacidad | Velocidad |
|--------|-------|---------------|---------|------------|-----------|
| **Transformers.js** | $0 | 3.8GB descarga | âœ… SÃ­ | ğŸ”’ Total | ğŸš€ğŸš€ğŸš€ |
| **Chrome AI** | $0 | 0 bytes (built-in) | âœ… SÃ­ | ğŸ”’ Total | ğŸš€ğŸš€ğŸš€ğŸš€ |
| **WebLLM** | $0 | 2-4GB descarga | âœ… SÃ­ | ğŸ”’ Total | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ |
| **Groq API** | $0 | 0 bytes | âŒ No | âš ï¸ Cloud | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ |
| **Together API** | $0 | 0 bytes | âŒ No | âš ï¸ Cloud | ğŸš€ğŸš€ğŸš€ |
| **HF Inference** | $0 | 0 bytes | âŒ No | âš ï¸ Cloud | ğŸš€ğŸš€ |

---

## ğŸ¯ RecomendaciÃ³n Final

### Para AINews Platform:

**Estrategia de 3 niveles**:

#### Nivel 1: Chrome AI (Si disponible)
- Cero setup, instantÃ¡neo
- Para: ResÃºmenes, clasificaciÃ³n, tags

#### Nivel 2: Transformers.js con opt-in
- Usuario elige descargar Phi-3.5-mini (3.8GB)
- Para: GeneraciÃ³n de cursos, anÃ¡lisis profundo
- Se guarda en cache del navegador (permanente)

#### Nivel 3: APIs Cloud (Fallback)
- Groq (ultra rÃ¡pido) â†’ Together â†’ HF
- Para: Usuarios que NO quieren descargar

**CÃ³digo de implementaciÃ³n**:

```typescript
// lib/ai/smart-llm.ts
export class SmartLLM {
  async generate(prompt: string, task: 'summary' | 'course' | 'classify') {
    // 1. Try Chrome AI (if available)
    if (task === 'summary' || task === 'classify') {
      try {
        const chromeAI = new ChromeAI();
        await chromeAI.initialize();
        return await chromeAI.generate(prompt);
      } catch {
        // Chrome AI not available
      }
    }
    
    // 2. Try Browser LLM (if downloaded)
    const browserLLM = getBrowserLLMFromCache();
    if (browserLLM && task === 'course') {
      return await browserLLM.generate(prompt);
    }
    
    // 3. Fallback to Cloud APIs
    return await this.cloudFallback(prompt);
  }
  
  async offerDownload() {
    // Show modal offering Phi-3.5-mini download
    // Only for users who generate courses frequently
  }
}
```

---

## ğŸš€ PrÃ³ximos Pasos

1. **Instalar Transformers.js**:
   ```bash
   npm install @xenova/transformers
   ```

2. **Crear componente de descarga**:
   ```tsx
   // components/ai/ModelDownloader.tsx
   ```

3. **Modificar generaciÃ³n de cursos**:
   ```tsx
   // app/[locale]/courses/generate/page.tsx
   ```

4. **Agregar fallback inteligente**:
   ```typescript
   // lib/ai/smart-llm.ts
   ```

Â¿Quieres que implemente la opciÃ³n de Transformers.js con Phi-3.5-mini? Es la **mÃ¡s prÃ¡ctica** porque:

- âœ… Usuario decide si descargar (opt-in)
- âœ… DespuÃ©s funciona 100% offline
- âœ… Calidad similar a GPT-3.5
- âœ… Cache permanente (descarga 1 sola vez)
- âœ… Fallback automÃ¡tico a Groq/Together si no descarga
